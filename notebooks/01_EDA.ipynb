{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b251d0bf",
   "metadata": {},
   "source": [
    "# Exploration Data Analysis notebook\n",
    "This notebook is used to exploration raw data from the settings.yaml for analysis and process it for backtesting.\n",
    "\n",
    "## Key Activates:\n",
    "    1. Download raw data from settings.yaml\n",
    "    2. Remove bad data from the dataset\n",
    "    3. add the techinal indictors\n",
    "    4. Plot graphs of these indictors\n",
    "    5. Save the new dataset into data/processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad7b986",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import yfinance as yf\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add src directory to path to import custom modules\n",
    "sys.path.append('../src')\n",
    "from data_pipeline import download_raw_data, preprocess_data, technical_indicators, save_data\n",
    "from signals import zscore_normalize, order_book_imbalance\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb822ed9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load configuration from settings.yaml\n",
    "with open('../configs/settings.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"Tickers: {config['data']['tickers']}\")\n",
    "print(f\"Date range: {config['data']['start_date']} to {config['data']['end_date']}\")\n",
    "print(f\"Moving average windows: {config['data']['moving_average_windows']}\")\n",
    "print(f\"Technical indicators: {config['data']['technical_indicators']}\")\n",
    "\n",
    "# Extract configuration parameters\n",
    "tickers = config['data']['tickers']\n",
    "start_date = config['data']['start_date']\n",
    "end_date = config['data']['end_date']\n",
    "ma_windows = config['data']['moving_average_windows']\n",
    "raw_data_path = config['paths']['raw_data_paths']\n",
    "processed_data_path = config['paths']['processed_data_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1bcd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Download raw data for all tickers\n",
    "print(\"Downloading raw data for all tickers...\")\n",
    "raw_data = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(f\"Downloading data for {ticker}...\")\n",
    "        data = download_raw_data(ticker, start_date, end_date)\n",
    "        raw_data[ticker] = data\n",
    "        print(f\"✓ Successfully downloaded {len(data)} records for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {ticker}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nCompleted downloading data for {len(raw_data)} tickers out of {len(tickers)} requested.\")\n",
    "\n",
    "# Display summary of downloaded data\n",
    "print(\"\\nData Summary:\")\n",
    "for ticker, data in raw_data.items():\n",
    "    print(f\"{ticker}: {len(data)} records from {data.index[0].date()} to {data.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26108fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Data Quality Assessment and Preprocessing\n",
    "print(\"Performing data quality assessment...\")\n",
    "\n",
    "# Check for missing values and data quality issues\n",
    "for ticker, data in raw_data.items():\n",
    "    print(f\"\\n{ticker} - Data Quality Check:\")\n",
    "    print(f\"  Shape: {data.shape}\")\n",
    "    print(f\"  Missing values: {data.isnull().sum().sum()}\")\n",
    "    print(f\"  Duplicate indices: {data.index.duplicated().sum()}\")\n",
    "    \n",
    "    # Check for obvious outliers in price data\n",
    "    if 'Close' in data.columns:\n",
    "        close_pct_change = data['Close'].pct_change()\n",
    "        extreme_moves = close_pct_change[abs(close_pct_change) > 0.2]\n",
    "        if len(extreme_moves) > 0:\n",
    "            print(f\"  Extreme price movements (>20%): {len(extreme_moves)}\")\n",
    "\n",
    "# Preprocess the data\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Preprocessing raw data...\")\n",
    "processed_data = {}\n",
    "\n",
    "for ticker, data in raw_data.items():\n",
    "    try:\n",
    "        processed_data[ticker] = preprocess_data(data.copy())\n",
    "        print(f\"✓ Preprocessed data for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error preprocessing {ticker}: {str(e)}\")\n",
    "\n",
    "print(f\"Successfully preprocessed {len(processed_data)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64745ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Basic Statistical Analysis\n",
    "print(\"Performing basic statistical analysis...\")\n",
    "\n",
    "# Create a combined DataFrame for analysis\n",
    "combined_data = pd.DataFrame()\n",
    "returns_data = pd.DataFrame()\n",
    "\n",
    "for ticker, data in processed_data.items():\n",
    "    if 'Close' in data.columns:\n",
    "        # Store closing prices\n",
    "        combined_data[ticker] = data['Close']\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = data['Close'].pct_change().dropna()\n",
    "        returns_data[ticker] = returns\n",
    "\n",
    "print(f\"Combined dataset shape: {combined_data.shape}\")\n",
    "print(f\"Returns dataset shape: {returns_data.shape}\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRICE STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(combined_data.describe())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RETURNS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "print(returns_data.describe())\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = returns_data.corr()\n",
    "print(f\"\\nAverage correlation between stocks: {correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50561886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Visualization - Price Charts and Returns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Normalized price evolution\n",
    "ax1 = axes[0, 0]\n",
    "normalized_prices = combined_data.div(combined_data.iloc[0]) * 100\n",
    "for ticker in normalized_prices.columns:\n",
    "    ax1.plot(normalized_prices.index, normalized_prices[ticker], label=ticker, alpha=0.8)\n",
    "ax1.set_title('Normalized Price Evolution (Base 100)', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Date')\n",
    "ax1.set_ylabel('Normalized Price')\n",
    "ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Returns distribution\n",
    "ax2 = axes[0, 1]\n",
    "returns_data.plot(kind='hist', bins=50, alpha=0.7, ax=ax2)\n",
    "ax2.set_title('Returns Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Daily Returns')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Plot 3: Correlation heatmap\n",
    "ax3 = axes[1, 0]\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, ax=ax3, cbar_kws={'shrink': 0.8})\n",
    "ax3.set_title('Returns Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Plot 4: Rolling volatility (30-day)\n",
    "ax4 = axes[1, 1]\n",
    "rolling_vol = returns_data.rolling(window=30).std() * np.sqrt(252)  # Annualized volatility\n",
    "for ticker in rolling_vol.columns:\n",
    "    ax4.plot(rolling_vol.index, rolling_vol[ticker], label=ticker, alpha=0.8)\n",
    "ax4.set_title('30-Day Rolling Volatility (Annualized)', fontsize=14, fontweight='bold')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.set_ylabel('Volatility')\n",
    "ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print volatility summary\n",
    "print(\"\\nVolatility Summary (Annualized):\")\n",
    "vol_summary = returns_data.std() * np.sqrt(252)\n",
    "for ticker, vol in vol_summary.items():\n",
    "    print(f\"{ticker}: {vol:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2f1163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Technical Indicators Analysis\n",
    "print(\"Computing technical indicators...\")\n",
    "\n",
    "# Enhanced technical indicators function\n",
    "def compute_comprehensive_indicators(df, ma_windows):\n",
    "    \"\"\"Compute comprehensive technical indicators\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Moving Averages\n",
    "    for window in ma_windows:\n",
    "        df_copy[f'SMA_{window}'] = df_copy['Close'].rolling(window=window).mean()\n",
    "        df_copy[f'EMA_{window}'] = df_copy['Close'].ewm(span=window).mean()\n",
    "    \n",
    "    # RSI\n",
    "    delta = df_copy['Close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    rs = gain / loss\n",
    "    df_copy['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    exp1 = df_copy['Close'].ewm(span=12).mean()\n",
    "    exp2 = df_copy['Close'].ewm(span=26).mean()\n",
    "    df_copy['MACD'] = exp1 - exp2\n",
    "    df_copy['MACD_Signal'] = df_copy['MACD'].ewm(span=9).mean()\n",
    "    df_copy['MACD_Histogram'] = df_copy['MACD'] - df_copy['MACD_Signal']\n",
    "    \n",
    "    # Bollinger Bands\n",
    "    df_copy['BB_Middle'] = df_copy['Close'].rolling(window=20).mean()\n",
    "    bb_std = df_copy['Close'].rolling(window=20).std()\n",
    "    df_copy['BB_Upper'] = df_copy['BB_Middle'] + (bb_std * 2)\n",
    "    df_copy['BB_Lower'] = df_copy['BB_Middle'] - (bb_std * 2)\n",
    "    df_copy['BB_Width'] = df_copy['BB_Upper'] - df_copy['BB_Lower']\n",
    "    df_copy['BB_Position'] = (df_copy['Close'] - df_copy['BB_Lower']) / (df_copy['BB_Upper'] - df_copy['BB_Lower'])\n",
    "    \n",
    "    # Average True Range (ATR)\n",
    "    high_low = df_copy['High'] - df_copy['Low']\n",
    "    high_close = np.abs(df_copy['High'] - df_copy['Close'].shift())\n",
    "    low_close = np.abs(df_copy['Low'] - df_copy['Close'].shift())\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    df_copy['ATR'] = true_range.rolling(window=14).mean()\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply technical indicators to all stocks\n",
    "technical_data = {}\n",
    "for ticker, data in processed_data.items():\n",
    "    print(f\"Computing indicators for {ticker}...\")\n",
    "    technical_data[ticker] = compute_comprehensive_indicators(data, ma_windows[:4])  # Use first 4 MA windows\n",
    "\n",
    "print(\"Technical indicators computed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cbaed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Technical Indicators Visualization\n",
    "# Select one stock for detailed technical analysis visualization\n",
    "sample_ticker = tickers[0]  # Use first ticker (AAPL)\n",
    "sample_data = technical_data[sample_ticker].copy()\n",
    "\n",
    "# Create comprehensive technical analysis charts\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 16))\n",
    "\n",
    "# Chart 1: Price with Moving Averages and Bollinger Bands\n",
    "ax1 = axes[0]\n",
    "ax1.plot(sample_data.index, sample_data['Close'], label='Close Price', linewidth=2)\n",
    "ax1.plot(sample_data.index, sample_data['SMA_25'], label='SMA 25', alpha=0.8)\n",
    "ax1.plot(sample_data.index, sample_data['SMA_50'], label='SMA 50', alpha=0.8)\n",
    "ax1.plot(sample_data.index, sample_data['BB_Upper'], label='BB Upper', linestyle='--', alpha=0.6)\n",
    "ax1.plot(sample_data.index, sample_data['BB_Lower'], label='BB Lower', linestyle='--', alpha=0.6)\n",
    "ax1.fill_between(sample_data.index, sample_data['BB_Upper'], sample_data['BB_Lower'], alpha=0.1)\n",
    "ax1.set_title(f'{sample_ticker} - Price, Moving Averages & Bollinger Bands', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Chart 2: RSI\n",
    "ax2 = axes[1]\n",
    "ax2.plot(sample_data.index, sample_data['RSI'], color='purple', linewidth=2)\n",
    "ax2.axhline(y=70, color='r', linestyle='--', alpha=0.7, label='Overbought (70)')\n",
    "ax2.axhline(y=30, color='g', linestyle='--', alpha=0.7, label='Oversold (30)')\n",
    "ax2.fill_between(sample_data.index, 30, 70, alpha=0.1, color='gray')\n",
    "ax2.set_title('Relative Strength Index (RSI)', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('RSI')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Chart 3: MACD\n",
    "ax3 = axes[2]\n",
    "ax3.plot(sample_data.index, sample_data['MACD'], label='MACD', linewidth=2)\n",
    "ax3.plot(sample_data.index, sample_data['MACD_Signal'], label='Signal', linewidth=2)\n",
    "ax3.bar(sample_data.index, sample_data['MACD_Histogram'], label='Histogram', alpha=0.6)\n",
    "ax3.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "ax3.set_title('MACD (Moving Average Convergence Divergence)', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('MACD')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Chart 4: Volume and ATR\n",
    "ax4 = axes[3]\n",
    "ax4_twin = ax4.twinx()\n",
    "ax4.bar(sample_data.index, sample_data['Volume'], alpha=0.6, color='blue', label='Volume')\n",
    "ax4_twin.plot(sample_data.index, sample_data['ATR'], color='red', linewidth=2, label='ATR')\n",
    "ax4.set_title('Volume and Average True Range (ATR)', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Volume', color='blue')\n",
    "ax4_twin.set_ylabel('ATR', color='red')\n",
    "ax4.set_xlabel('Date')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "# Add legends\n",
    "ax4.legend(loc='upper left')\n",
    "ax4_twin.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print technical indicator summary\n",
    "print(f\"\\nTechnical Indicators Summary for {sample_ticker}:\")\n",
    "print(f\"Current RSI: {sample_data['RSI'].iloc[-1]:.1f}\")\n",
    "print(f\"Current MACD: {sample_data['MACD'].iloc[-1]:.3f}\")\n",
    "print(f\"Current BB Position: {sample_data['BB_Position'].iloc[-1]:.2f} (0=lower band, 1=upper band)\")\n",
    "print(f\"Current ATR: {sample_data['ATR'].iloc[-1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe7943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Cross-Asset Analysis and Signal Generation\n",
    "print(\"Performing cross-asset analysis...\")\n",
    "\n",
    "# Create signals using custom functions from signals.py\n",
    "print(\"Generating trading signals...\")\n",
    "\n",
    "# Calculate z-scores for mean reversion signals\n",
    "zscore_data = pd.DataFrame()\n",
    "for ticker in tickers:\n",
    "    if ticker in technical_data:\n",
    "        close_prices = technical_data[ticker]['Close']\n",
    "        # Calculate rolling z-score (mean reversion signal)\n",
    "        rolling_mean = close_prices.rolling(window=60).mean()\n",
    "        rolling_std = close_prices.rolling(window=60).std()\n",
    "        zscore_data[ticker] = (close_prices - rolling_mean) / rolling_std\n",
    "\n",
    "# Apply z-score normalization using custom function\n",
    "normalized_signals = pd.DataFrame()\n",
    "for ticker in zscore_data.columns:\n",
    "    try:\n",
    "        normalized_signals[ticker] = zscore_normalize(zscore_data[ticker].dropna())\n",
    "    except:\n",
    "        print(f\"Warning: Could not normalize {ticker}\")\n",
    "\n",
    "print(f\"Generated signals for {len(normalized_signals.columns)} assets\")\n",
    "\n",
    "# Visualization of signals\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Plot 1: Z-scores for all assets\n",
    "ax1 = axes[0]\n",
    "for ticker in zscore_data.columns[:4]:  # Plot first 4 for clarity\n",
    "    ax1.plot(zscore_data.index, zscore_data[ticker], label=ticker, alpha=0.8)\n",
    "ax1.axhline(y=2, color='r', linestyle='--', alpha=0.7, label='Overbought (+2σ)')\n",
    "ax1.axhline(y=-2, color='g', linestyle='--', alpha=0.7, label='Oversold (-2σ)')\n",
    "ax1.fill_between(zscore_data.index, -2, 2, alpha=0.1, color='gray')\n",
    "ax1.set_title('60-Day Rolling Z-Scores (Mean Reversion Signals)', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Z-Score')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Signal distribution\n",
    "ax2 = axes[1]\n",
    "zscore_data.hist(bins=50, alpha=0.7, ax=ax2)\n",
    "ax2.set_title('Distribution of Z-Score Signals', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Z-Score')\n",
    "ax2.set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Signal analysis\n",
    "extreme_signals = zscore_data[abs(zscore_data) > 2].count()\n",
    "print(\"\\nExtreme Signals Analysis (|Z-Score| > 2):\")\n",
    "for ticker, count in extreme_signals.items():\n",
    "    if count > 0:\n",
    "        total_obs = len(zscore_data[ticker].dropna())\n",
    "        percentage = (count / total_obs) * 100\n",
    "        print(f\"{ticker}: {count} signals ({percentage:.1f}% of observations)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dd985b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Statistical Arbitrage Opportunity Analysis\n",
    "print(\"Analyzing statistical arbitrage opportunities...\")\n",
    "\n",
    "# Pairs analysis - find potential pairs for statistical arbitrage\n",
    "from itertools import combinations\n",
    "\n",
    "# Calculate correlation and cointegration analysis\n",
    "pairs_analysis = []\n",
    "\n",
    "for ticker1, ticker2 in combinations(tickers, 2):\n",
    "    if ticker1 in returns_data.columns and ticker2 in returns_data.columns:\n",
    "        # Get price series\n",
    "        price1 = combined_data[ticker1].dropna()\n",
    "        price2 = combined_data[ticker2].dropna()\n",
    "        \n",
    "        # Align dates\n",
    "        common_dates = price1.index.intersection(price2.index)\n",
    "        if len(common_dates) > 100:  # Ensure sufficient data\n",
    "            price1_aligned = price1[common_dates]\n",
    "            price2_aligned = price2[common_dates]\n",
    "            \n",
    "            # Calculate correlation\n",
    "            correlation = price1_aligned.corr(price2_aligned)\n",
    "            \n",
    "            # Simple spread analysis\n",
    "            spread = price1_aligned - price2_aligned\n",
    "            spread_std = spread.std()\n",
    "            spread_mean = spread.mean()\n",
    "            \n",
    "            pairs_analysis.append({\n",
    "                'Pair': f\"{ticker1}-{ticker2}\",\n",
    "                'Correlation': correlation,\n",
    "                'Spread_Mean': spread_mean,\n",
    "                'Spread_Std': spread_std,\n",
    "                'Spread_CV': spread_std / abs(spread_mean) if spread_mean != 0 else np.inf\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame and sort by correlation\n",
    "pairs_df = pd.DataFrame(pairs_analysis)\n",
    "pairs_df = pairs_df.sort_values('Correlation', ascending=False)\n",
    "\n",
    "print(\"Top 10 Most Correlated Pairs:\")\n",
    "print(pairs_df.head(10))\n",
    "\n",
    "# Visualize top pairs\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot top 2 most correlated pairs\n",
    "top_pairs = pairs_df.head(2)\n",
    "for i, (_, row) in enumerate(top_pairs.iterrows()):\n",
    "    ticker1, ticker2 = row['Pair'].split('-')\n",
    "    \n",
    "    ax = axes[i//2, i%2]\n",
    "    \n",
    "    # Normalize prices to same scale\n",
    "    price1_norm = combined_data[ticker1] / combined_data[ticker1].iloc[0] * 100\n",
    "    price2_norm = combined_data[ticker2] / combined_data[ticker2].iloc[0] * 100\n",
    "    \n",
    "    ax.plot(price1_norm.index, price1_norm, label=ticker1, linewidth=2)\n",
    "    ax.plot(price2_norm.index, price2_norm, label=ticker2, linewidth=2)\n",
    "    ax.set_title(f'Normalized Prices: {ticker1} vs {ticker2}\\nCorrelation: {row[\"Correlation\"]:.3f}', \n",
    "                 fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Normalized Price (Base 100)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot spreads for top 2 pairs\n",
    "for i, (_, row) in enumerate(top_pairs.iterrows()):\n",
    "    ticker1, ticker2 = row['Pair'].split('-')\n",
    "    \n",
    "    ax = axes[1, i]\n",
    "    \n",
    "    spread = combined_data[ticker1] - combined_data[ticker2]\n",
    "    ax.plot(spread.index, spread, color='red', linewidth=2)\n",
    "    ax.axhline(y=spread.mean(), color='blue', linestyle='--', alpha=0.7, label='Mean')\n",
    "    ax.axhline(y=spread.mean() + 2*spread.std(), color='orange', linestyle='--', alpha=0.7, label='+2σ')\n",
    "    ax.axhline(y=spread.mean() - 2*spread.std(), color='orange', linestyle='--', alpha=0.7, label='-2σ')\n",
    "    ax.fill_between(spread.index, \n",
    "                   spread.mean() - 2*spread.std(), \n",
    "                   spread.mean() + 2*spread.std(), \n",
    "                   alpha=0.1, color='orange')\n",
    "    ax.set_title(f'Price Spread: {ticker1} - {ticker2}', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Price Spread ($)')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12872fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Risk Metrics and Portfolio Analysis\n",
    "print(\"Computing risk metrics...\")\n",
    "\n",
    "# Calculate risk metrics for each asset\n",
    "risk_metrics = pd.DataFrame()\n",
    "\n",
    "for ticker in tickers:\n",
    "    if ticker in returns_data.columns:\n",
    "        returns = returns_data[ticker].dropna()\n",
    "        \n",
    "        # Basic risk metrics\n",
    "        risk_metrics.loc[ticker, 'Annualized_Return'] = returns.mean() * 252\n",
    "        risk_metrics.loc[ticker, 'Annualized_Volatility'] = returns.std() * np.sqrt(252)\n",
    "        risk_metrics.loc[ticker, 'Sharpe_Ratio'] = (returns.mean() * 252) / (returns.std() * np.sqrt(252))\n",
    "        \n",
    "        # Downside metrics\n",
    "        negative_returns = returns[returns < 0]\n",
    "        risk_metrics.loc[ticker, 'Downside_Deviation'] = negative_returns.std() * np.sqrt(252)\n",
    "        risk_metrics.loc[ticker, 'Sortino_Ratio'] = (returns.mean() * 252) / (negative_returns.std() * np.sqrt(252))\n",
    "        \n",
    "        # Maximum drawdown\n",
    "        cumulative_returns = (1 + returns).cumprod()\n",
    "        rolling_max = cumulative_returns.expanding().max()\n",
    "        drawdown = (cumulative_returns - rolling_max) / rolling_max\n",
    "        risk_metrics.loc[ticker, 'Max_Drawdown'] = drawdown.min()\n",
    "        \n",
    "        # VaR (95% confidence)\n",
    "        risk_metrics.loc[ticker, 'VaR_95'] = np.percentile(returns, 5)\n",
    "        risk_metrics.loc[ticker, 'CVaR_95'] = returns[returns <= np.percentile(returns, 5)].mean()\n",
    "\n",
    "# Display risk metrics\n",
    "print(\"\\nRisk Metrics Summary:\")\n",
    "print(risk_metrics.round(4))\n",
    "\n",
    "# Risk-Return visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Risk-Return scatter\n",
    "ax1 = axes[0, 0]\n",
    "scatter = ax1.scatter(risk_metrics['Annualized_Volatility'], \n",
    "                     risk_metrics['Annualized_Return'],\n",
    "                     s=100, alpha=0.7, c=risk_metrics['Sharpe_Ratio'], \n",
    "                     cmap='viridis')\n",
    "ax1.set_xlabel('Annualized Volatility')\n",
    "ax1.set_ylabel('Annualized Return')\n",
    "ax1.set_title('Risk-Return Profile', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Add ticker labels\n",
    "for ticker in risk_metrics.index:\n",
    "    ax1.annotate(ticker, \n",
    "                (risk_metrics.loc[ticker, 'Annualized_Volatility'], \n",
    "                 risk_metrics.loc[ticker, 'Annualized_Return']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "plt.colorbar(scatter, ax=ax1, label='Sharpe Ratio')\n",
    "\n",
    "# Plot 2: Sharpe vs Sortino Ratio\n",
    "ax2 = axes[0, 1]\n",
    "ax2.scatter(risk_metrics['Sharpe_Ratio'], risk_metrics['Sortino_Ratio'], \n",
    "           s=100, alpha=0.7)\n",
    "ax2.set_xlabel('Sharpe Ratio')\n",
    "ax2.set_ylabel('Sortino Ratio')\n",
    "ax2.set_title('Sharpe vs Sortino Ratio', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "for ticker in risk_metrics.index:\n",
    "    ax2.annotate(ticker, \n",
    "                (risk_metrics.loc[ticker, 'Sharpe_Ratio'], \n",
    "                 risk_metrics.loc[ticker, 'Sortino_Ratio']),\n",
    "                xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "\n",
    "# Plot 3: Maximum Drawdown\n",
    "ax3 = axes[1, 0]\n",
    "risk_metrics['Max_Drawdown'].plot(kind='bar', ax=ax3, color='red', alpha=0.7)\n",
    "ax3.set_title('Maximum Drawdown by Asset', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Max Drawdown')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: VaR and CVaR\n",
    "ax4 = axes[1, 1]\n",
    "x_pos = np.arange(len(risk_metrics.index))\n",
    "width = 0.35\n",
    "ax4.bar(x_pos - width/2, risk_metrics['VaR_95'], width, label='VaR (95%)', alpha=0.7)\n",
    "ax4.bar(x_pos + width/2, risk_metrics['CVaR_95'], width, label='CVaR (95%)', alpha=0.7)\n",
    "ax4.set_xlabel('Assets')\n",
    "ax4.set_ylabel('Daily Return')\n",
    "ax4.set_title('Value at Risk and Conditional VaR', fontsize=14, fontweight='bold')\n",
    "ax4.set_xticks(x_pos)\n",
    "ax4.set_xticklabels(risk_metrics.index, rotation=45)\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Portfolio analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PORTFOLIO ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Equal weight portfolio\n",
    "equal_weights = np.ones(len(returns_data.columns)) / len(returns_data.columns)\n",
    "portfolio_returns = (returns_data * equal_weights).sum(axis=1)\n",
    "\n",
    "# Portfolio metrics\n",
    "portfolio_metrics = {\n",
    "    'Portfolio Annualized Return': portfolio_returns.mean() * 252,\n",
    "    'Portfolio Annualized Volatility': portfolio_returns.std() * np.sqrt(252),\n",
    "    'Portfolio Sharpe Ratio': (portfolio_returns.mean() * 252) / (portfolio_returns.std() * np.sqrt(252)),\n",
    "    'Portfolio Max Drawdown': ((1 + portfolio_returns).cumprod() / (1 + portfolio_returns).cumprod().expanding().max() - 1).min()\n",
    "}\n",
    "\n",
    "print(\"Equal-Weight Portfolio Metrics:\")\n",
    "for metric, value in portfolio_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Diversification benefit\n",
    "individual_risk = (risk_metrics['Annualized_Volatility'] * equal_weights).sum()\n",
    "portfolio_risk = portfolio_metrics['Portfolio Annualized Volatility']\n",
    "diversification_ratio = individual_risk / portfolio_risk\n",
    "\n",
    "print(f\"\\nDiversification Ratio: {diversification_ratio:.2f}\")\n",
    "print(f\"Risk Reduction: {(1 - 1/diversification_ratio)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbce4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Save Processed Data\n",
    "print(\"Saving processed datasets...\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(raw_data_path, exist_ok=True)\n",
    "os.makedirs(processed_data_path, exist_ok=True)\n",
    "\n",
    "# Save raw data\n",
    "print(\"Saving raw data...\")\n",
    "for ticker, data in raw_data.items():\n",
    "    filename = os.path.join(raw_data_path, f\"{ticker}_raw.csv\")\n",
    "    save_data(data, filename)\n",
    "    print(f\"✓ Saved raw data for {ticker}\")\n",
    "\n",
    "# Save processed data with technical indicators\n",
    "print(\"\\nSaving processed data with technical indicators...\")\n",
    "for ticker, data in technical_data.items():\n",
    "    filename = os.path.join(processed_data_path, f\"{ticker}_processed.csv\")\n",
    "    save_data(data, filename)\n",
    "    print(f\"✓ Saved processed data for {ticker}\")\n",
    "\n",
    "# Save combined datasets\n",
    "print(\"\\nSaving combined datasets...\")\n",
    "\n",
    "# Combined prices\n",
    "combined_data.to_csv(os.path.join(processed_data_path, \"combined_prices.csv\"))\n",
    "print(\"✓ Saved combined prices dataset\")\n",
    "\n",
    "# Combined returns\n",
    "returns_data.to_csv(os.path.join(processed_data_path, \"combined_returns.csv\"))\n",
    "print(\"✓ Saved combined returns dataset\")\n",
    "\n",
    "# Risk metrics\n",
    "risk_metrics.to_csv(os.path.join(processed_data_path, \"risk_metrics.csv\"))\n",
    "print(\"✓ Saved risk metrics\")\n",
    "\n",
    "# Pairs analysis\n",
    "pairs_df.to_csv(os.path.join(processed_data_path, \"pairs_analysis.csv\"), index=False)\n",
    "print(\"✓ Saved pairs analysis\")\n",
    "\n",
    "# Z-score signals\n",
    "zscore_data.to_csv(os.path.join(processed_data_path, \"zscore_signals.csv\"))\n",
    "print(\"✓ Saved z-score signals\")\n",
    "\n",
    "# Summary statistics\n",
    "summary_stats = {\n",
    "    'dataset_info': {\n",
    "        'tickers': tickers,\n",
    "        'date_range': f\"{start_date} to {end_date}\",\n",
    "        'total_observations': len(combined_data),\n",
    "        'number_of_assets': len(tickers)\n",
    "    },\n",
    "    'data_quality': {\n",
    "        'successful_downloads': len(raw_data),\n",
    "        'successful_processing': len(technical_data),\n",
    "        'average_correlation': correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open(os.path.join(processed_data_path, \"eda_summary.yaml\"), 'w') as f:\n",
    "    yaml.dump(summary_stats, f, default_flow_style=False)\n",
    "print(\"✓ Saved EDA summary\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EDA COMPLETE!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Data saved to: {processed_data_path}\")\n",
    "print(f\"Raw data files: {len(raw_data)}\")\n",
    "print(f\"Processed data files: {len(technical_data)}\")\n",
    "print(f\"Analysis period: {start_date} to {end_date}\")\n",
    "print(f\"Assets analyzed: {', '.join(tickers)}\")\n",
    "print(\"\\nKey findings:\")\n",
    "print(f\"- Average correlation between assets: {correlation_matrix.values[np.triu_indices_from(correlation_matrix.values, k=1)].mean():.3f}\")\n",
    "print(f\"- Best Sharpe ratio: {risk_metrics['Sharpe_Ratio'].max():.3f} ({risk_metrics['Sharpe_Ratio'].idxmax()})\")\n",
    "print(f\"- Highest volatility: {risk_metrics['Annualized_Volatility'].max():.1%} ({risk_metrics['Annualized_Volatility'].idxmax()})\")\n",
    "print(f\"- Most correlated pair: {pairs_df.iloc[0]['Pair']} (correlation: {pairs_df.iloc[0]['Correlation']:.3f})\")\n",
    "\n",
    "print(\"\\nReady for backtesting and strategy development!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
